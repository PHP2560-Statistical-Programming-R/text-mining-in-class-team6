<<<<<<< HEAD
word_counts_all %>%
# Take the top 5 words for each sentiment
top_n(5) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
# Set up the plot with aes()
ggplot(aes(word, n, mapping = sentiment)) +
geom_col(show.legend = FALSE, width = 0.5) +
facet_wrap(~ sentiment, scales = "free", nrow = 5) +
coord_flip() +
ggtitle("Top 5 words for each sentiment") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/top10_pos_and_neg.png')
word_counts_pos_neg %>%
# Group by sentiment
group_by(sentiment) %>%
# Take the top 10 for each sentiment
top_n(10) %>%
ungroup() %>%
# Make word a factor in order of n
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
# Make a bar chart with geom_col()
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 10 positive & negative words") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/change_over_time.png')
sentiment_by_time %>%
# Filter for positive and negative words
filter(sentiment %in% c("positive", "negative")) %>%
# Count by date, sentiment, and total_words
count(time, sentiment, total_words) %>%
ungroup() %>%
mutate(percent = n / total_words) %>%
# Set up the plot with aes()
ggplot(aes(time, percent, mapping = sentiment)) +
geom_line(aes(colour = sentiment, group = sentiment), size = 1) +
geom_smooth(method = "lm", se = FALSE, lty = 2) +
expand_limits(y = 0) +
ggtitle("Sentiment change over time") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
names <- c(
"dplyr",
"stringr",
"rebus",
"lubridate",
"ggplot2",
"tidytext",
"wordcloud"
)
for(name in names) {
if (!(name %in% installed.packages()))
install.packages(name, repos="http://cran.us.r-project.org")
library(name, character.only=TRUE)
}
# export the data manually from "http://www.trumptwitterarchive.com/archive" and save as a csv file
# read the csv file
DT_all_tweets <- read.csv("data/Donald_Trump_twitter.csv")
# get the date and the time of the most recent tweet
most_recent_tweet <- DT_all_tweets[1, 3]
# print the date and the time of the most recent tweet
paste0("The date and the time of the most recent tweet: ", most_recent_tweet)
date_and_time <- DT_all_tweets$created_at %>%
str_split(pattern = " ", simplify = TRUE) %>%
as_tibble() %>%
rename(date = V1, time = V2) %>%
mutate(date = as.Date(date, "%m/%d/%y"))
date <- date_and_time$date %>%
str_match(
pattern = capture(one_or_more(DGT)) %R%
"-" %R% capture(one_or_more(DGT)) %R%
"-" %R% capture(one_or_more(DGT))
) %>%
as_tibble() %>%
rename(date = V1, year = V2, month = V3, day = V4)
clean_data <- DT_all_tweets %>%
cbind(date) %>%
filter(is_retweet == "FALSE") %>%
select(text, date, year, month, day)
clean_data$text <- as.character(clean_data$text)
tidy_words <- clean_data %>%
as.tbl() %>%
unnest_tokens(word, text) %>%
filter(!(word %in% c(
"t.co",
"http",
"https",
"amp",
"twitter",
"android",
"web",
"client",
"realdonaldtrump"
)))
word_counts_all <- tidy_words %>%
# Implement sentiment analysis using the "nrc" lexicon
inner_join(get_sentiments("nrc")) %>%
# Count by word and sentiment
count(word, sentiment) %>%
# Group by sentiment
group_by(sentiment)
word_counts_pos_neg <- tidy_words %>%
# Implement sentiment analysis using the "nrc" lexicon
inner_join(get_sentiments("nrc")) %>%
filter(sentiment %in% c("positive", "negative")) %>%
# Count by word and sentiment
count(word, sentiment)
sentiment_by_time <- tidy_words %>%
# Define a new column using floor_date()
mutate(time = floor_date(as_datetime(date), unit = "6 months")) %>%
# Group by date
group_by(time) %>%
mutate(total_words = n()) %>%
ungroup() %>%
# Implement sentiment analysis using the NRC lexicon
inner_join(get_sentiments("nrc"))
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2015)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
dir.create("graph/", showWarnings = FALSE)
png('graph/top10.png')
tidy_words %>%
anti_join(stop_words) %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
top_n(10) %>%
ggplot(aes(word, n, fill = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
ggtitle("Top 10 expressions") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/wordcloud.png')
par(mfrow = c(2,2))
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2017)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
text(x=0.5, y= 0, "2017")
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2016)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2"), main = "P"))
text(x=0.5, y=0, "2016")
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2015)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
text(x=0.5, y=0.2, "2015")
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2014)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
text(x=0.5, y=0.2, "2014")
dev.off()
png('graph/top5_sentiments.png')
word_counts_all %>%
# Take the top 5 words for each sentiment
top_n(5) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
# Set up the plot with aes()
ggplot(aes(word, n, mapping = sentiment)) +
geom_col(show.legend = FALSE, width = 0.5) +
facet_wrap(~ sentiment, scales = "free", nrow = 5) +
coord_flip() +
ggtitle("Top 5 words for each sentiment") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/top10_pos_and_neg.png')
word_counts_pos_neg %>%
# Group by sentiment
group_by(sentiment) %>%
# Take the top 10 for each sentiment
top_n(10) %>%
ungroup() %>%
# Make word a factor in order of n
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
# Make a bar chart with geom_col()
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 10 positive & negative words") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/change_over_time.png')
sentiment_by_time %>%
# Filter for positive and negative words
filter(sentiment %in% c("positive", "negative")) %>%
# Count by date, sentiment, and total_words
count(time, sentiment, total_words) %>%
ungroup() %>%
mutate(percent = n / total_words) %>%
# Set up the plot with aes()
ggplot(aes(time, percent, mapping = sentiment)) +
geom_line(aes(colour = sentiment, group = sentiment), size = 1) +
geom_smooth(method = "lm", se = FALSE, lty = 2) +
expand_limits(y = 0) +
ggtitle("Sentiment change over time") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
remove.packages("lubridate", lib="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
names <- c(
"dplyr",
"stringr",
"rebus",
"lubridate",
"ggplot2",
"tidytext",
"wordcloud"
)
for(name in names) {
if (!(name %in% installed.packages()))
install.packages(name, repos="http://cran.us.r-project.org")
library(name, character.only=TRUE)
}
install.packages(name, repos = "http://cran.us.r-project.org")
## clean all output from previous runs of scripts
unlink("graph", recursive = TRUE) # where graphs are stores
# now re-create the results directory
dir.create(file.path("graph"), showWarnings = FALSE)
## run all scripts
source("00_getting_packages.R")       # install and prepare the requiered packages
source("01_getting_data.R")           # read and store the data
source("02_cleaning.R")               # clean the data
source("03_sentiment_analysis.R")     # conduct sentiment analysis
source("04_visualization.R")          # create plot and graphs
# rmarkdown::render("paper.Rmd", output_format = "html_document")
install.packages(name, repos = "http://cran.us.r-project.org")
## clean all output from previous runs of scripts
unlink("graph", recursive = TRUE) # where graphs are stores
# now re-create the results directory
dir.create(file.path("graph"), showWarnings = FALSE)
## run all scripts
source("00_getting_packages.R")       # install and prepare the requiered packages
source("01_getting_data.R")           # read and store the data
source("02_cleaning.R")               # clean the data
source("03_sentiment_analysis.R")     # conduct sentiment analysis
source("04_visualization.R")          # create plot and graphs
# rmarkdown::render("paper.Rmd", output_format = "html_document")
dir.create("graph/", showWarnings = FALSE)
png('graph/top10.png')
tidy_words %>%
anti_join(stop_words) %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
top_n(10) %>%
ggplot(aes(word, n, fill = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
ggtitle("Top 10 expressions") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/wordcloud.png')
par(mfrow = c(2,2))
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2017)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
text(x=0.5, y= 0, "2017")
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2016)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2"), main = "P"))
text(x=0.5, y=0, "2016")
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2015)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
text(x=0.5, y=0.2, "2015")
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2014)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
text(x=0.5, y=0.2, "2014")
dev.off()
png('graph/top5_sentiments.png')
word_counts_all %>%
# Take the top 5 words for each sentiment
top_n(5) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
# Set up the plot with aes()
ggplot(aes(word, n, mapping = sentiment)) +
geom_col(show.legend = FALSE, width = 0.5) +
facet_wrap(~ sentiment, scales = "free", nrow = 5) +
coord_flip() +
ggtitle("Top 5 words for each sentiment") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/top10_pos_and_neg.png')
word_counts_pos_neg %>%
# Group by sentiment
group_by(sentiment) %>%
# Take the top 10 for each sentiment
top_n(10) %>%
ungroup() %>%
# Make word a factor in order of n
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
# Make a bar chart with geom_col()
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 10 positive & negative words") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/change_over_time.png')
sentiment_by_time %>%
# Filter for positive and negative words
filter(sentiment %in% c("positive", "negative")) %>%
# Count by date, sentiment, and total_words
count(time, sentiment, total_words) %>%
ungroup() %>%
mutate(percent = n / total_words) %>%
# Set up the plot with aes()
ggplot(aes(time, percent, mapping = sentiment)) +
geom_line(aes(colour = sentiment, group = sentiment), size = 1) +
geom_smooth(method = "lm", se = FALSE, lty = 2) +
expand_limits(y = 0) +
ggtitle("Sentiment change over time") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
install.packages("lubridate", lib="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
remove.packages("dplyr", lib="~/Library/R/3.3/library")
remove.packages("stringr", lib="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
remove.packages("wordcloud", lib="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
## clean all output from previous runs of scripts
unlink("graph", recursive = TRUE) # where graphs are stores
# now re-create the results directory
dir.create(file.path("graph"), showWarnings = FALSE)
## run all scripts
source("00_getting_packages.R")       # install and prepare the requiered packages
source("01_getting_data.R")           # read and store the data
source("02_cleaning.R")               # clean the data
source("03_sentiment_analysis.R")     # conduct sentiment analysis
source("04_visualization.R")          # create plot and graphs
# rmarkdown::render("paper.Rmd", output_format = "html_document")
## clean all output from previous runs of scripts
unlink("graph", recursive = TRUE) # where graphs are stores
# now re-create the results directory
dir.create(file.path("graph"), showWarnings = FALSE)
## run all scripts
source("00_getting_packages.R")       # install and prepare the requiered packages
source("01_getting_data.R")           # read and store the data
source("02_cleaning.R")               # clean the data
source("03_sentiment_analysis.R")     # conduct sentiment analysis
source("04_visualization.R")          # create plot and graphs
# rmarkdown::render("paper.Rmd", output_format = "html_document")
## clean all output from previous runs of scripts
unlink("graph", recursive = TRUE) # where graphs are stores
## run all scripts
source("00_getting_packages.R")       # install and prepare the requiered packages
source("01_getting_data.R")           # read and store the data
source("02_cleaning.R")               # clean the data
source("03_sentiment_analysis.R")     # conduct sentiment analysis
source("04_visualization.R")          # create plot and graphs
# rmarkdown::render("paper.Rmd", output_format = "html_document")
source("00_getting_packages.R")       # install and prepare the requiered packages
source("00_getting_packages.R")       # install and prepare the requiered packages
source("01_getting_data.R")           # read and store the data
source("02_cleaning.R")               # clean the data
source("03_sentiment_analysis.R")     # conduct sentiment analysis
source("04_visualization.R")          # create plot and graphs
source("04_visualization.R")          # create plot and graphs
dir.create("graph/", showWarnings = FALSE)
png('graph/top10.png')
tidy_words %>%
anti_join(stop_words) %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
top_n(10) %>%
ggplot(aes(word, n, fill = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
ggtitle("Top 10 expressions") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/wordcloud.png')
par(mfrow = c(2,2))
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2017)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
text(x=0.5, y= 0, "2017")
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2016)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2"), main = "P"))
text(x=0.5, y=0, "2016")
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2015)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
text(x=0.5, y=0.2, "2015")
tidy_words %>%
anti_join(stop_words) %>%
filter(year == 2014)  %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
with(wordcloud(word, scale = c(1.5, .2), n, max.words = 100, random.order = F, random.color = F, colors = brewer.pal(8, "Dark2")))
text(x=0.5, y=0.2, "2014")
dev.off()
png('graph/top5_sentiments.png')
word_counts_all %>%
# Take the top 5 words for each sentiment
top_n(5) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
# Set up the plot with aes()
ggplot(aes(word, n, mapping = sentiment)) +
geom_col(show.legend = FALSE, width = 0.5) +
facet_wrap(~ sentiment, scales = "free", nrow = 5) +
coord_flip() +
ggtitle("Top 5 words for each sentiment") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/top10_pos_and_neg.png')
word_counts_pos_neg %>%
# Group by sentiment
group_by(sentiment) %>%
# Take the top 10 for each sentiment
top_n(10) %>%
ungroup() %>%
# Make word a factor in order of n
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
# Make a bar chart with geom_col()
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 10 positive & negative words") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
png('graph/change_over_time.png')
sentiment_by_time %>%
# Filter for positive and negative words
filter(sentiment %in% c("positive", "negative")) %>%
# Count by date, sentiment, and total_words
count(time, sentiment, total_words) %>%
ungroup() %>%
mutate(percent = n / total_words) %>%
# Set up the plot with aes()
ggplot(aes(time, percent, mapping = sentiment)) +
geom_line(aes(colour = sentiment, group = sentiment), size = 1) +
geom_smooth(method = "lm", se = FALSE, lty = 2) +
expand_limits(y = 0) +
ggtitle("Sentiment change over time") +
theme(plot.title = element_text(hjust = 0.5))
dev.off()
## run all scripts
source("00_getting_packages.R")       # install and prepare the requiered packages
source("01_getting_data.R")           # read and store the data
source("02_cleaning.R")               # clean the data
source("03_sentiment_analysis.R")     # conduct sentiment analysis
source("04_visualization.R")          # create plot and graphs
# rmarkdown::render("paper.Rmd", output_format = "html_document")
## run all scripts
source("00_getting_packages.R")       # install and prepare the requiered packages
source("01_getting_data.R")           # read and store the data
source("02_cleaning.R")               # clean the data
source("03_sentiment_analysis.R")     # conduct sentiment analysis
source("04_visualization.R")          # create plot and graphs
# rmarkdown::render("paper.Rmd", output_format = "html_document")
dir.create("graph/", showWarnings = FALSE)
png('graph/top10.png')
tidy_words %>%
anti_join(stop_words) %>%
count(word, sort = T) %>%
mutate(word = reorder(word, n)) %>%
top_n(10) %>%
ggplot(aes(word, n, fill = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
ggtitle("Top 10 expressions") +
theme(plot.title = element_text(hjust = 0.5))
source("04_visualization.R")          # create plot and graphs
=======
word_counts <- sentiments_bing %>%
count(max_rating, word) %>%
group_by(max_rating) %>%
mutate(rating_counts = sum(n)) %>%
ungroup() %>%
mutate(prop = n / rating_counts)
top <- word_counts %>%
group_by(max_rating) %>%
top_n(n = 10, wt = prop) %>%
ungroup() %>%
arrange(desc(prop))
ggplot(top, aes(reorder(word, prop), prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free") +
coord_flip() +
ylab("") +
xlab("Top Ten Words") +
ggtitle("Top Ten Words used in Ted Talks by rating")
#######what are the most common sentiments within ted talks grouped by their most popular rating#########
sentiment_plot <- sentiments_nrc %>%
filter(!sentiment %in% c("positive", "negative")) %>%
count(max_rating, sentiment) %>%
group_by(max_rating) %>%
mutate(sentiment_count = sum(n)) %>%
ungroup() %>%
mutate(prop = n/ sentiment_count)
ggplot(sentiment_plot, aes(sentiment, prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free", labeller = ) +
coord_flip() +
ylab("") +
xlab("Sentiment") +
ggtitle("Distribution of Sentiments of Words used in Ted Talks by Rating")
### what is the distribution of ratings given to ted talks ###
rating_dist <- transcripts_clean %>%
distinct(name, max_rating) %>%
group_by(max_rating) %>%
count()
ggplot(rating_dist, aes(reorder(max_rating, n), n, fill = max_rating)) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
xlab("Max Rating") +
ylab("Count") +
ggtitle("Distribution of Max Ratings given to all Ted Talks")
ggplot(word_freq, aes(reorder(word,n), n, fill = word)) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(word_freq, aes(reorder(word,n), n)) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#### what are the most used words among all ted talks###
word_freq <- sentiments_bing %>%
group_by(word) %>%
count() %>%
ungroup() %>%
top_n(50)
ggplot(word_freq, aes(reorder(word,n), n)) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
###decide to remove the word "Like" from further analysis -- it does not have a sentiment and has almost
###double the frequency of the next most common word
#### what are the most used words among all ted talks###
word_freq <- sentiments_bing %>%
group_by(word) %>%
count() %>%
ungroup() %>%
top_n(50)
ggplot(word_freq, aes(reorder(word,n), n, fill = "mistyrose")) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
###decide to remove the word "Like" from further analysis -- it does not have a sentiment and has almost
###double the frequency of the next most common word
#### what are the most used words among all ted talks###
word_freq <- sentiments_bing %>%
group_by(word) %>%
count() %>%
ungroup() %>%
top_n(50)
ggplot(word_freq, aes(reorder(word,n), n, fill = "lavenderblush")) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
###decide to remove the word "Like" from further analysis -- it does not have a sentiment and has almost
###double the frequency of the next most common word
#### what are the most used words among all ted talks###
word_freq <- sentiments_bing %>%
group_by(word) %>%
count() %>%
ungroup() %>%
top_n(50)
ggplot(word_freq, aes(reorder(word,n), n, fill = 10)) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
###decide to remove the word "Like" from further analysis -- it does not have a sentiment and has almost
###double the frequency of the next most common word
#### what are the most used words among all ted talks###
word_freq <- sentiments_bing %>%
group_by(word) %>%
count() %>%
ungroup() %>%
top_n(50)
ggplot(word_freq, aes(reorder(word,n), n, fill = "#9999CC")) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
###decide to remove the word "Like" from further analysis -- it does not have a sentiment and has almost
###double the frequency of the next most common word
#### what are the most used words among all ted talks###
word_freq <- sentiments_bing %>%
group_by(word) %>%
count() %>%
ungroup() %>%
top_n(50)
ggplot(word_freq, aes(reorder(word,n), n, fill = 10)) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
###decide to remove the word "Like" from further analysis -- it does not have a sentiment and has almost
###double the frequency of the next most common word
View(sentiments_nrc)
#######what are the most common sentiments within ted talks grouped by their most popular rating#########
sentiment_plot <- sentiments_nrc %>%
filter(!word %in% c("like")) %>%
filter(!sentiment %in% c("positive", "negative")) %>%
count(max_rating, sentiment) %>%
group_by(max_rating) %>%
mutate(sentiment_count = sum(n)) %>%
ungroup() %>%
mutate(prop = n/ sentiment_count)
ggplot(sentiment_plot, aes(sentiment, prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free", labeller = ) +
coord_flip() +
ylab("") +
xlab("Sentiment") +
ggtitle("Distribution of Sentiments of Words used in Ted Talks by Rating")
View(sentiment_plot)
##### what are the top ten words used within ted talks grouped by their most popular rating #####
word_counts <- sentiments_bing %>%
filter(!word %in% c("like")) %>%
count(max_rating, word) %>%
group_by(max_rating) %>%
mutate(rating_counts = sum(n)) %>%
ungroup() %>%
mutate(prop = n / rating_counts)
top <- word_counts %>%
group_by(max_rating) %>%
top_n(n = 10, wt = prop) %>%
ungroup() %>%
arrange(desc(prop))
ggplot(top, aes(reorder(word, prop), prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free") +
coord_flip() +
ylab("") +
xlab("Top Ten Words") +
ggtitle("Top Ten Words used in Ted Talks by rating")
#######what are the most common sentiments within ted talks grouped by their most popular rating#########
sentiment_plot <- sentiments_nrc %>%
filter(!word %in% c("like", "right")) %>%
filter(!sentiment %in% c("positive", "negative")) %>%
count(max_rating, sentiment) %>%
group_by(max_rating) %>%
mutate(sentiment_count = sum(n)) %>%
ungroup() %>%
mutate(prop = n/ sentiment_count)
ggplot(sentiment_plot, aes(sentiment, prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free", labeller = ) +
coord_flip() +
ylab("") +
xlab("Sentiment") +
ggtitle("Distribution of Sentiments of Words used in Ted Talks by Rating")
##### what are the top ten words used within ted talks grouped by their most popular rating #####
word_counts <- sentiments_bing %>%
filter(!word %in% c("like","right")) %>%
count(max_rating, word) %>%
group_by(max_rating) %>%
mutate(rating_counts = sum(n)) %>%
ungroup() %>%
mutate(prop = n / rating_counts)
top <- word_counts %>%
group_by(max_rating) %>%
top_n(n = 10, wt = prop) %>%
ungroup() %>%
arrange(desc(prop))
ggplot(top, aes(reorder(word, prop), prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free") +
coord_flip() +
ylab("") +
xlab("Top Ten Words") +
ggtitle("Top Ten Words used in Ted Talks by rating")
#######what's the average sentiment//how positive/negative is each talk grouped by popular rating#########
pos_neg <- sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
count(max_rating, sentiment) %>%
group_by(max_rating) %>%
mutate(sentiment_count = sum(n)) %>%
ungroup() %>%
mutate(prop = n / sentiment_count)
ggplot(pos_neg, aes(sentiment, prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free", labeller = ) +
coord_flip() +
ggtitle("Frequency of Postive + Negative words used in Ted Talks by rating") +
xlab("") +
ylab("")
words_used <- transcripts_clean %>%
count(max_rating, word) %>%
group_by(max_rating) %>%
mutate(word_count = sum(n)) %>%
top_n(n = 10, wt = word_count) %>%
ungroup() %>%
mutate(prop = n / word_count)
ggplot(words_used, aes(word, prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free") +
coord_flip()
View(full_data)
### what is the distribution of ratings given to ted talks ###
rating_dist <- transcripts_clean %>%
distinct(name, max_rating) %>%
group_by(max_rating) %>%
count()
ggplot(rating_dist, aes(reorder(max_rating, n), n, fill = max_rating)) +
geom_col(show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
xlab("Max Rating") +
ylab("Count") +
ggtitle("Distribution of Max Ratings given to all Ted Talks")
#######what's the average sentiment//how positive/negative is each talk grouped by popular rating#########
pos_neg <- sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
count(max_rating, sentiment) %>%
group_by(max_rating) %>%
mutate(sentiment_count = sum(n)) %>%
ungroup() %>%
mutate(prop = n / sentiment_count)
ggplot(pos_neg, aes(sentiment, prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free", labeller = ) +
coord_flip() +
ggtitle("Frequency of Postive + Negative words used in Ted Talks by rating") +
xlab("") +
ylab("")
View(sentiments_bing)
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious"))
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious", "Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment + max_rating, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious", "Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious", "Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
facet_wrap(~ max_rating, scales = "free")
sentiments_bing %>%
filter(max_rating %in% c("Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment + max_rating, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Courageous", "OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment + max_rating, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Courageous", "OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
sentiments_bing %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Obnoxious")
sentiments_bing %>%
filter(max_rating %in% c("Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Courageous")
sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("Obnoxious")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Obnoxious")
sentiments_nrc %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Courageous")
sentiments_nrc %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("Obnoxious")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Obnoxious")
sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Positive + Negative Sentiment when Rating is OK")
sentiments_nrc %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
knitr::opts_chunk$set(echo = TRUE)
ci.mean(n=32, xbar=0.91, s2=0.2, alpha=0.01, distr="t")
>>>>>>> d40f80d6f7c1d0ed91a90dcbf1dce4ac85913067
