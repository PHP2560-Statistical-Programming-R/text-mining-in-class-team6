xlab("Max Rating") +
ylab("Count") +
ggtitle("Distribution of Max Ratings given to all Ted Talks")
#######what's the average sentiment//how positive/negative is each talk grouped by popular rating#########
pos_neg <- sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
count(max_rating, sentiment) %>%
group_by(max_rating) %>%
mutate(sentiment_count = sum(n)) %>%
ungroup() %>%
mutate(prop = n / sentiment_count)
ggplot(pos_neg, aes(sentiment, prop, fill = max_rating)) +
geom_col(show.legend = FALSE) +
facet_wrap(~max_rating, scales = "free", labeller = ) +
coord_flip() +
ggtitle("Frequency of Postive + Negative words used in Ted Talks by rating") +
xlab("") +
ylab("")
View(sentiments_bing)
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious"))
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious", "Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment + max_rating, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious", "Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious", "Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
facet_wrap(~ max_rating, scales = "free")
sentiments_bing %>%
filter(max_rating %in% c("Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment + max_rating, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Courageous", "OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment + max_rating, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("Courageous", "OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
sentiments_bing %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
sentiments_bing %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
sentiments_bing %>%
filter(max_rating %in% c("Obnoxious")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Obnoxious")
sentiments_bing %>%
filter(max_rating %in% c("Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Courageous")
sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("Obnoxious")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Obnoxious")
sentiments_nrc %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("Courageous")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Courageous")
sentiments_nrc %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("Obnoxious")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is Obnoxious")
sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
sentiments_bing %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Positive + Negative Sentiment when Rating is OK")
sentiments_nrc %>%
filter(!word %in% c("like", "right")) %>%
filter(max_rating %in% c("OK")) %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment when Rating is OK")
knitr::opts_chunk$set(echo = TRUE)
ci.mean(n=32, xbar=0.91, s2=0.2, alpha=0.01, distr="t")
### Load required packages:
library(stringr)
library(tidyverse)
library(tidytext)
library(lubridate)
library(ggrepel)
library(rebus)
### Read in metadata:
ted_main <- read_csv("~/skewl/text_analysis/text-mining-in-class-team6/TedTalks/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("/TedTalks/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("TedTalks/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("skewl/text_analysis/textproject/TedTalks/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("text_analysis/textproject/TedTalks/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("textproject/TedTalks/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("TedTalks/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("TedTalks/data/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("TedTalks/data/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("TedTalks/data/ted_main.csv")
### Load required packages:
library(stringr)
library(tidyverse)
library(tidytext)
library(lubridate)
library(ggrepel)
library(rebus)
### Read in metadata:
ted_main <- read_csv("TedTalks/data/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("TedTalks/data/ted_main.csv")
### Read in metadata:
ted_main <- read_csv("data/ted_main.csv")
### Read in lecture transcripts:
transcripts <- read_csv("~/skewl/text_analysis/text-mining-in-class-team6/TedTalks/transcripts.csv")
### Read in metadata:
ted_main <- read_csv("data/ted_main.csv")
### Read in lecture transcripts:
transcripts <- read_csv("data/transcripts.csv")
### Fix url formatting so the two data sets match:
transcripts$url = str_replace_all(transcripts$url, pattern = "\r", replacement = "")
### Read in metadata:
ted_main <- read_csv("data/ted_main.csv")
### Read in lecture transcripts:
transcripts <- read_csv("data/transcripts.csv")
### Fix url formatting so the two data sets match:
transcripts$url = str_replace_all(transcripts$url, pattern = "\r", replacement = "")
### Combine data sets for sentiment analysis:
combined = inner_join(ted_main, transcripts, by = "url")
### Remove words that are not part of the talks:
for (i in 1:nrow(combined)){
combined[i, "transcript"] =
str_replace_all(
combined[i, "transcript"],
pattern = "\\([^()]+\\)",
" "
)
}
### Change to token format for sentiment analysis:
transcripts_clean = combined %>% unnest_tokens(output = word, input = transcript)
### Add a total words per talk column:
transcripts_clean = transcripts_clean %>%
group_by(name) %>%
mutate(wordcount = n()) %>%
ungroup()
### Join with Bing lexicon:
transcripts_bing = transcripts_clean %>%
inner_join(get_sentiments("bing"))
### Next, count the number of positive words in each talk
### Then, divide by the total words to obtain the proportion
### Then, output the top 20 most positive talks
transcripts_bing %>%
filter(sentiment == "positive") %>%
count(name, sentiment, wordcount) %>%
mutate(positive_p = n / wordcount) %>%
arrange(desc(positive_p)) %>%
top_n(20)
### First find a good cutoff point for word count.
### Start by dividing data into 2.5% quantiles:
temp = transcripts_bing %>%
select(wordcount) %>%
unlist() %>%
quantile(probs = seq(0, 1, .025))
temp
tempplot = transcripts_bing %>%
group_by(name) %>%
arrange(wordcount) %>%
filter(wordcount <= 992) %>%
ggplot(aes(y = wordcount, x = seq_along(wordcount))) +
geom_smooth() +
ggtitle("Determine a Word Count Cutoff")+
xlab("Performances") +
ylab("Word Count") +
theme_minimal()
tempplot2 = tempplot +
geom_hline(yintercept = 510, color = "red", size = 1.5) +
geom_text(aes(10000, 560, label = "count filter = 510"), color = "red")
tempplot2
### Most artist performances are trimmed by filtering the data for word counts > 510
transcripts_clean %>%
group_by(name) %>%
filter(wordcount > 510) %>%
summarise(words = mean(wordcount)) %>%
arrange(words)
transcripts_bing %>%
filter(sentiment == "positive") %>%
filter(wordcount > 510) %>%
count(name, sentiment, wordcount) %>%
mutate(positive_p = n / wordcount) %>%
arrange(desc(positive_p)) %>%
top_n(20)
###Count by word and sentiment
###Then, group by sentiment
###Then, plot the top 20 words for each sentiment
transcripts_bing %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
ggtitle("Top 20 Words That Contributed to Sentiment")
### Join with NRC Lexicon:
transcripts_nrc = transcripts_clean %>%
inner_join(get_sentiments("nrc"))
transcripts_nrc %>%
# Count by word and sentiment
count(word, sentiment) %>%
# Group by sentiment
group_by(sentiment) %>%
# Take the top 20 words for each sentiment
top_n(5) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
# Set up the plot with aes()
ggplot(aes(x = word, y= n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ylab("count") +
facet_wrap(~ sentiment, scales = "free") +
coord_flip() +
scale_y_continuous(breaks = seq(0, 4000, 2000))
### Convert Unix time to a date:
transcripts_bing = transcripts_bing %>%
mutate(year = year(as.Date(as.POSIXct(transcripts_bing$film_date, origin="1970-01-01"))))
###Group by year and count the total sentiment words
bing_by_year <- transcripts_bing %>%
group_by(year) %>%
mutate(total_words = n()) %>%
ungroup()
###Create variable "p"
bing_by_year %>%
count(year, sentiment, total_words) %>%
mutate(p = n / total_words) %>%
ggplot(aes(x = year, y = p, color = sentiment)) +
geom_line(size = 1.5) +
geom_smooth(method = "lm", se = FALSE, lty = 3) +
expand_limits(y = 0) +
xlab("Year") +
ylab("Proportion of Sentiment Word")
### Want to add top tag per year
### First extract tags
### Then find top tag per year
tagsdata = bing_by_year %>%
distinct(title, tags, year)%>%
unnest_tokens(output = tag, input = tags) %>%
group_by(year, tag) %>%
count() %>%
ungroup() %>%
group_by(year) %>%
slice(which.max(n)) %>%
select(year, tag)
bing_by_year = bing_by_year %>% inner_join(tagsdata) %>% rename(yeartoptag = tag)
bing_by_year %>%
count(year, sentiment, total_words, yeartoptag) %>%
mutate(p = n / total_words) %>%
ggplot(aes(x = year, y = p, color = sentiment)) +
geom_line(size = 1.5) +
geom_smooth(method = "lm", se = FALSE, lty = 3) +
expand_limits(y = 0) +
xlab("Year") +
ylab("Proportion of Sentiment Words") +
geom_text(aes(x = year, y = .5, label = yeartoptag), angle = 90, color = "hotpink4")
##read in main data set
ted_main <- read_csv("TedTalks/data/ted_main.csv")
##read in transcripts
transcripts <- read_csv("TedTalks/data/transcripts.csv")
###combine data sets
full_data <- inner_join(ted_main, transcripts, by = "url")
###remove any text in the transcript that is surrounded by parenthesis
for (i in 1:nrow(full_data)){
full_data[i, "transcript"] =
str_replace_all(
full_data[i, "transcript"],
pattern = "\\([^()]+\\)",
" "
)
}
for(i in 1:nrow(full_data)) {
rating_string <- str_sub(full_data$ratings[i], 2,-2)
rating_vector <- unlist(strsplit(rating_string, split="}"))
names <- str_extract_all(rating_vector, pattern = "'name': '" %R% one_or_more(WRD) %R% optional('-') %R%
one_or_more(WRD), simplify = T)
names <- str_replace(names, pattern = "'name': '", "")
counts <- str_extract_all(rating_vector, pattern = "'count': " %R% one_or_more(DGT), simplify = T)
counts <- str_replace(counts, pattern = "'count': ", "")
full_data$max_rating[i] <- names[which.max(counts)]
}
save(full_data, file = "TedTalks/data/full_data.Rda")
transcripts_clean <- full_data %>% unnest_tokens(word, transcript)
save(transcripts_clean, file = "TedTalks/data/transcripts_clean.Rda")
sentiments_bing <- transcripts_clean %>% inner_join(get_sentiments("bing"))
save(sentiments_bing, file = "TedTalks/data/sentiments_bing.Rda")
sentiments_nrc <- transcripts_clean %>% inner_join(get_sentiments("nrc")) %>% filter(!word %in% c("like"))
save(sentiments_nrc, file = "TedTalks/data/sentiments_nrc.Rda")
